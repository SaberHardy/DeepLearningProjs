NOTES:
> Dropout: used for correct the over-fitting
The solution using Dropout is:
    This is the idea behind dropout. To break up these conspiracies, 
    we randomly drop out some fraction of a layer's input units 
    every step of training, making it much harder for the network 
    to learn those spurious patterns in the training data.



> When adding dropout, we need to increase the number of units in Dense layer.
> 
> 
